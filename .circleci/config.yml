version: 2.1
orbs:
  slack: circleci/slack@4.1.1
  aws-eks: circleci/aws-eks@0.2.0
  aws-ecr: circleci/aws-ecr@3.1.0
  kubernetes: circleci/kubernetes@0.3.0

parameters:
  cluster-name:
    type: string
    default: "capstone-employee"

commands:
  destroy-environment:
    description: Destroy backend and frontend stacks.
    steps:      
      - run:
          name: Destroy environment
          when: on_fail
          command: |
            if ! [ -x "$(command -v aws)" ]; then
            pip install awscli
            else
            echo "AWS CLI already installed"
            fi
            aws s3 rm s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive
            aws cloudformation delete-stack --stack-name frontend-${CIRCLE_WORKFLOW_ID:0:7}
            aws cloudformation delete-stack --stack-name backend-${CIRCLE_WORKFLOW_ID:0:7}  
  
  revert-migrations:
    description: Revert the last migration if successfully run in the current workflow.   
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            SUCCESS=$(curl -H "token: ${CIRCLE_WORKFLOW_ID:0:7}" \
            --request GET https://api.memstash.io/values/migration-${CIRCLE_WORKFLOW_ID:0:7})
            echo "************** SUCCESS = $SUCCESS"
            if(( $SUCCESS==1 )); 
            then
             cd backend
             npm install
             npm run migrations:revert
            fi      
            
jobs:
  build-frontend:
    docker:
      - image: circleci/node:13.8.0
        environment:
          baseUrl: http://local:9095
    steps:
      - checkout
      - restore_cache:
          keys: 
            - frontend-build-${CIRCLE_WORKFLOW_ID:0:7}
      - run:
          name: Build front-end
          command: |            
            cd angular8-crud-demo      
            npm install
            npm run build --prod 
      - save_cache:
          paths:
            - angular8-crud-demo/node_modules
            - angular8-crud-demo/dist
          key: frontend-build-${CIRCLE_WORKFLOW_ID:0:7}

  build-backend:
    docker:
      - image: circleci/openjdk:11-jdk        
    steps:
    - checkout
    - restore_cache:
        keys:
          - v1-dependencies-{{ checksum "springboot-crud-api/pom.xml" }}
          # fallback to using the latest cache if no exact match is found
          - v1-dependencies-    
    - run:
        name: Back-end build
        command: |
          cd springboot-crud-api          
          source $BASH_ENV     
          mvn clean install -DskipTests
    - save_cache:
          paths: 
            - ~/.m2
          key: v1-dependencies-{{ checksum "springboot-crud-api/pom.xml" }}
  
  test-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: 
            - frontend-build-${CIRCLE_WORKFLOW_ID:0:7}
      - run:
          name: Test front-end
          no_output_timeout: 20m
          command: |
            cd angular8-crud-demo     
            npm run test
                
  test-backend:
    docker:
      - image: circleci/openjdk:11-jdk
    steps:
    - checkout
    - restore_cache:
        keys: 
          - v1-dependencies-{{ checksum "springboot-crud-api/pom.xml" }}
          # fallback to using the latest cache if no exact match is found
          - v1-dependencies-    
    - run:
        name: Test Back-end
        command: |
          cd springboot-crud-api
          mvn dependency:go-offline
          echo 'export DATABASE_URL=jdbc:mysql://capstone.caejyuh3eggl.eu-west-1.rds.amazonaws.com:3306/capstone' >> $BASH_ENV
          echo 'export DATABASE_USER=admin' >> $BASH_ENV
          echo 'export DATABASE_PASSWORD=Capstone21' >> $BASH_ENV
          source $BASH_ENV     
          mvn test
          
  scan-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: 
            - frontend-build-${CIRCLE_WORKFLOW_ID:0:7}
      - run:
          name: Scan frontend
          command: |
            cd frontend
            npm update        
            npm audit fix --audit-level=critical

  scan-backend:
    docker:
      - image: 'circleci/openjdk:11-jdk'
    steps:
      - checkout
      - restore_cache:
          keys: 
            - v1-dependencies-{{ checksum "springboot-crud-api/pom.xml" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies- 
      - run:
          name: Analyze on SonarCloud
          command: |
            cd springboot-crud-api
            mvn dependency:go-offline        
            mvn verify sonar:sonar

   

  build_and_push:
    docker:
      - image: circleci/openjdk:11-jdk
    steps:
      - checkout
      - setup_remote_docker
      - run:
          name: Setup common environment variables
          command: |
            echo 'export ECR_REPOSITORY_NAME="${AWS_RESOURCE_NAME_PREFIX}"' >> $BASH_ENV
            echo 'export FULL_IMAGE_NAME="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${ECR_REPOSITORY_NAME}:${CIRCLE_SHA1}"' >> $BASH_ENV
      - restore_cache:
          keys: 
            - v1-dependencies-{{ checksum "springboot-crud-api/pom.xml" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies- 
      - run:
          name: Install dependencies for AWS
          command: |
            sudo apt-get update
            sudo apt-get -y -qq install awscli
            sudo apt-get install python3-pip
            sudo pip3 install --upgrade awscli
      - run:
          name: Build image
          command: |
            cd springboot-crud-api
            mvn dependency:go-offline
            mvn package
            docker build -t $FULL_IMAGE_NAME .
      - run:
          name: Push image
          command: |
            eval $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)
            docker push $FULL_IMAGE_NAME

  create-eks-cluster:
    executor: aws-eks/python3
    steps:
      - checkout
      - aws-eks/create-cluster:
          cluster-name: << pipeline.parameters.cluster-name >>
          aws-region: $AWS_DEFAULT_REGION
          nodegroup-name: ng-1
          node-type: t2.medium
          nodes: 1

  deploy-infrastructure:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Installing dependencies
          command: |
            yum install -y tar gzip curl
      - run:
          name: Ensure front-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=capstone \
              --stack-name frontend-${CIRCLE_WORKFLOW_ID:0:7} \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}" 

  deploy-application:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
      docker-image-name:
        description: |
          Name of the docker image to be deployed
        type: string
      version-info:
        description: |
          App version information
        type: string
      aws-region:
        description: |
          AWS region
        type: string
        default: ""
    steps:
      - checkout
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
          install-kubectl: true
          aws-region: $AWS_DEFAULT_REGION
      - run:
          name: Create deployment
          command: |
            cd springboot-crud-api
            cat deployment.yaml
            sed -i "s/AWS_ACCOUNT_ID/$AWS_ACCOUNT_ID/g" deployment.yaml
            sed -i "s/CIRCLE_SHA1/$CIRCLE_SHA1/g" deployment.yaml
            sed -i "s/DATABASE_USER_VALUE/$DATABASE_USER/g" deployment.yaml
            sed -i "s/DBNAME/$DBNAME/g" deployment.yaml
            sed -i "s/DATABASE_PASSWORD_VALUE/$DATABASE_PASSWORD/g" deployment.yaml
            cat deployment.yaml
            kubectl apply -f deployment.yaml
      - run:
          name: Create service
          command: |
            cd springboot-crud-api
            kubectl apply -f service.yaml

  deploy-frontend:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            yum install gzip tar python3-pip curl -y
            curl -sL https://rpm.nodesource.com/setup_10.x | bash -
            yum install nodejs npm -y
      - run:
          name: Get backend url
          command: |
            BACKEND_URL=$(curl -H "token: 12345" --request GET https://api.memstash.io/values/external_ip)
            echo "Checking external IP: ${BACKEND_URL}"
            cd angular8-crud-demo
            cat src/environments/environment.prod.ts
            sed -i "s/BACKEND_URL/$BACKEND_URL/g" 'src/environments/environment.prod.ts'
            cat src/environments/environment.prod.ts
            npm install
            npm run build --prod
            ls -alh
      - run:
          name: Deploy frontend objects
          command: |
            cd angular8-crud-demo
            cat src/environments/environment.prod.ts
            aws s3 cp ./dist s3://capstone-c2771e4 --recursive

  test-application:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
      aws-region:
        description: |
          AWS region
        type: string
        default: ""
      expected-version-info:
        description: |
          Expected app version (this is used for testing that the
          correct version has been deployed)
        type: string
    steps:
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
          install-kubectl: true
          aws-region: $AWS_DEFAULT_REGION
      - run:
          name: Wait for service to be ready
          command: |
            kubectl get pods
            kubectl get services
            sleep 30
            for attempt in {1..20}; do
              EXTERNAL_IP=$(kubectl get service employee | awk '{print $4}' | tail -n1)
              echo "Checking external IP: ${EXTERNAL_IP}"
              curl -H "Content-Type: text/plain" \
               -H "token: ${CIRCLE_WORKFLOW_ID:0:7}" \
               --request PUT \
               --data ${EXTERNAL_IP} \
               https://api.memstash.io/values/external_ip
              if [ -n "${EXTERNAL_IP}" ] && [ -z $(echo "${EXTERNAL_IP}" | grep "pending") ]; then
                break
              fi
              echo "Waiting for external IP to be ready: ${EXTERNAL_IP}"
              sleep 10
            done
            sleep 180
            curl -s --retry 10 "http://$EXTERNAL_IP/api/employees" | grep "<< parameters.expected-version-info >>"

  smoke-test:
    docker:
      - image: python:3.7-alpine3.11 
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |           
            apk add --update curl tar gzip python3 py3-pip            
      - run:
          name: Frontend smoke test.
          command: |
            FRONTEND_URL="http://capstone-${CIRCLE_WORKFLOW_ID:0:7}.s3-website-eu-west-1.amazonaws.com/"            
            if curl -s ${FRONTEND_URL} | grep "Welcome"
            then
              return 0
            else
              return 1
            fi

  undeploy-application:
      executor: aws-eks/python3
      parameters:
        cluster-name:
          description: |
            Name of the EKS cluster
          type: string
        aws-region:
          description: |
            AWS region
          type: string
          default: ""
      steps:
        - aws-eks/update-kubeconfig-with-authenticator:
            cluster-name: << parameters.cluster-name >>
            install-kubectl: true
            aws-region: << parameters.aws-region >>
        - kubernetes/delete-resource:
            resource-types: "deployment,service"
            label-selector: "app=employee"
            wait: true
        - run:
            name: Check on pod status
            command: |
              kubectl get pods 

workflows:
  default:
    jobs:      
      -  deploy-frontend           
      #- build-frontend
      #- build-backend
      #- test-frontend:
      #    requires: [build-frontend]
      #- test-backend:
       #   requires: [build-backend]
      #- scan-backend:
       #   context: SonarCloud
        #  requires: [build-backend]
      #- undeploy-application:
       #   cluster-name: "capstone-employee"
        #  aws-region: $AWS_DEFAULT_REGION
      #- build_and_push:
       #   requires: [undeploy-application]
      #- create-eks-cluster:
       #   requires: [build_and_push]
      #- aws-eks/create-cluster:
       #   cluster-name: capstone-employee
        #  aws-region: $AWS_DEFAULT_REGION
         # requires:
          #  - build_and_push
      #- deploy-application:
       #   cluster-name: "capstone-employee"
        #  aws-region: $AWS_DEFAULT_REGION
         # docker-image-name: "${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${ECR_REPOSITORY_NAME}:${CIRCLE_SHA1}"
          #version-info: "${CIRCLE_SHA1}"
          #requires: [build_and_push]
      #- test-application:
       #   name: test-application
        #  cluster-name: "capstone-employee"
        #  aws-region: $AWS_DEFAULT_REGION
         # expected-version-info: "${CIRCLE_SHA1}"
          #requires:
          #  - deploy-application
      #- undeploy-application:
       #   cluster-name: "capstone-employee"
        #  aws-region: $AWS_DEFAULT_REGION
         # requires:
         #   - test-application
      #- aws-eks/delete-cluster:
       #   cluster-name: "capstone-employee"
        #  aws-region: $AWS_DEFAULT_REGION
         # wait: true
          #requires:
           # - undeploy-application